---
title: "Model Validation and Assessment"
author: |
  | Dawn Nekorchuk, Michael Wimberly, and EPIDEMIA Team Members
  | Department of Geography and Environmental Sustainability, University of Oklahoma
  | dawn.nekorchuk@ou.edu; mcwimberly@ou.edu
date: "Updated `r format(Sys.time(), '%B %d, %Y')`"
output:
  rmarkdown::html_vignette:
    fig_caption: yes
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '2'
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: 2
vignette: |
  %\VignetteIndexEntry{Model Validation and Assessment} 
  %\VignetteEncoding{UTF-8} 
  %\VignetteEngine{knitr::rmarkdown}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


# Running Model Validation and Assessment

Validation and assessment have been built into the epidemiar package in the function `run_validation()` for on-demand evaluation for any historical period. 

Evaluation can be made for one through n-week ahead predictions, and include comparisons with two naïve models: persistence of last known value, and average cases from that week of the year.

Building validation into the early warning system provides more opportunities to learn about the model via the validation results. Locations where the models perform best can be identified with geographical grouping-level results. 

With on-demand implementation and time-range flexibility, one can also investigate how accuracy changes over time, which is of particular interest in places like Ethiopia with changing patterns and declining trends due to anti-malarial programs. 


## Specific Arguments

The `run_validation()` function takes 4 arguments, plus all the `run_epidemia()` arguments. 

* `week_start`: The week to begin validation, can be built with `epidemiar::make_date_yw()` and isoyear and isoweek numbers (or epiweeks, with appropriate modifications).
* `total_weeks`: The number of weeks from `week_start` to run the validation.
* `week_ahead`: To validate 1 through _n_-week ahead predictions. 
* `skill_test`: TRUE/FALSE on whether to also run the naïve models for comparison: Persistence - last known value carried forward n-appropriate weeks, and Average Week - the average cases from that week of the week (per geographic grouping). 


## Other Arguments & Adjustments

The `run_validation()` function will call `run_epidemia()`, so it will also take all the arguments for that function. The user does not need to modify any of these arguments (e.g. event detection settings, `forecast_future`), as `run_validation()` will automatically handle all of thse adjustments. 

It is envisioned that users can take their usual script for running EPIDEMIA forecasts, and simply sub in the validation function with those 4 validation settings for doing model assessments. 


# Validation Output

## Statistics

Validation statistics included Mean Squared Error (`MSE`), Mean Absolute Error (`MAE`), and proportion of observations that fell inside the prediction intervals (`prop_interval`). 

Results will be returned summarized at the model level and also at the geographic grouping level. 

## Format

Results are returned in a list, by model (name of the base model and the naïve models if run with skill test comparison). 

* `validation_overall`: overall results 
* `validation_grouping`: results per geographic grouping


## Helper functions

There are a few helper functions to assist with the validation results. 

* `view_overall_validations(validations)`: Will pull out just the overall results per model. 
* `save_overall_validations(validations, save_file)`: Will save out just the overall results to a csv file with a column for the model type. 
* `save_geog_validations(validations, save_file)`: Will save out just the geographic grouping-level results to a csv file with a column for the model type. 
